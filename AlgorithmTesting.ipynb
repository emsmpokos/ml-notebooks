{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a43bb2-7a58-48ea-9a8b-4abef677b858",
   "metadata": {},
   "source": [
    "### 0.0.2 Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e522c-a00b-4ba6-a9c8-edd757beef34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from prettytable import PrettyTable\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "class CustomAlgorithm:\n",
    "    #insert custom algorithm\n",
    "\n",
    "def optimize_and_evaluate(X_, y_, X_test, y_test, use_customalgorithm=True):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_, y_, test_size=0.3, random_state=42)\n",
    "    \n",
    "    def objective(trial):\n",
    "        params_trial = {\n",
    "            'n_estimators': trial.suggest_categorical('n_estimators', [100, 150, 200, 300]),\n",
    "            'max_depth': trial.suggest_categorical('max_depth', [None, 5, 10, 20, 30]),\n",
    "            'min_samples_split': trial.suggest_categorical('min_samples_split', [2, 5, 10]),\n",
    "            'min_samples_leaf': trial.suggest_categorical('min_samples_leaf', [1, 2, 4]),\n",
    "            'max_features': trial.suggest_categorical('max_features', [None, 'sqrt', 'log2']),\n",
    "            'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "            'random_state': 42\n",
    "        }\n",
    "        estimator = RandomForestRegressor(**params_trial)\n",
    "        \n",
    "        if use_customalgorithm:\n",
    "            custom = trial.suggest_categorical('k', [3, 4, 5, 6])\n",
    "            model = CustomAlgorithm(estimator)\n",
    "        else:\n",
    "            model = estimator\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "        yhat_valid = model.predict(X_valid)\n",
    "        mse = mean_squared_error(y_valid, yhat_valid)\n",
    "        return mse\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=100)\n",
    "    best_params = study.best_params\n",
    "    best_mse = study.best_value\n",
    "\n",
    "    if use_customalgorithm:\n",
    "        custom = best_params.pop()\n",
    "        estimator = RandomForestRegressor(**best_params)\n",
    "        model = CustomAlgorithm(estimator)\n",
    "    else:\n",
    "        estimator = RandomForestRegressor(**best_params)\n",
    "        model = estimator\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    yhat_train = model.predict(X_train)\n",
    "    yhat_test = model.predict(X_test)\n",
    "\n",
    "\n",
    "    #The main goal of the optuna study is to get the result with the lowest MSE, there might be other results with better SMAPE/MAE\n",
    "    #but I am choosing to work with MSE here\n",
    "    smape = 100 * np.mean(2 * np.abs(yhat_test - y_test) / (np.abs(yhat_test) + np.abs(y_test)))\n",
    "    mae = mean_absolute_error(y_test, yhat_test)\n",
    "    mse = mean_squared_error(y_test, yhat_test)\n",
    "\n",
    "    return best_params, best_mse, smape, mae, mse\n",
    "\n",
    "# Function to run the Monte Carlo simulation\n",
    "def monte_carlo_simulation(n_iterations, n_samples=100):\n",
    "    results = {'CustomAlgorithm': {'MAE': [], 'MSE': [], 'SMAPE': []}, 'Regressor': {'MAE': [], 'MSE': [], 'SMAPE': []}}\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        X, y = make_regression(n_samples=n_samples, n_features=5, noise=50, random_state=i)\n",
    "\n",
    "        # Adding interaction terms to generate some variance in the dataset\n",
    "        interaction_terms = X[:, :5] * X[:, :5]  # Adjust the range to cover all features\n",
    "        X = np.hstack([X, interaction_terms])\n",
    "\n",
    "        # Modifying noise and introduce outliers for the same reason as above\n",
    "        np.random.seed(i)\n",
    "        noise = np.random.normal(0, 20, size=y.shape)\n",
    "        y += noise\n",
    "        outliers = np.random.choice(np.arange(len(y)), size=5, replace=False)\n",
    "        y[outliers] *= 3 \n",
    "\n",
    "        # Applying shifting variations (which was the main thing I did in the first dataset I made)\n",
    "        # and it seems like a good idea to change the amount of points and the shift values to introduce variance\n",
    "        points_to_shift = random.randint(3, 8)                     \n",
    "        shift_values = np.arange(1, 2.0, 0.25)\n",
    "        shift = random.choice(shift_values)\n",
    "\n",
    "        min_indices_y = np.argpartition(y, points_to_shift)[:points_to_shift]\n",
    "        max_indices_y = np.argpartition(y, -points_to_shift)[-points_to_shift:]\n",
    "\n",
    "        X[min_indices_y] -= shift\n",
    "        y[min_indices_y] -= shift\n",
    "        X[max_indices_y] += shift\n",
    "        y[max_indices_y] += shift\n",
    "\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "        _, _, smape_cal, mae_cal, mse_cal = optimize_and_evaluate(X_train, y_train, X_test, y_test, use_customalgorithm=True)\n",
    "        _, _, smape_rf, mae_rf, mse_rf = optimize_and_evaluate(X_train, y_train, X_test, y_test, use_customalgorithm=False)\n",
    "\n",
    "        # Store results\n",
    "        results['CustomAlgorithm']['MAE'].append(mae_cal)\n",
    "        results['CustomAlgorithm']['MSE'].append(mse_cal)\n",
    "        results['CustomAlgorithm']['SMAPE'].append(smape_cal)\n",
    "\n",
    "        results['Regressor']['MAE'].append(mae_rf)\n",
    "        results['Regressor']['MSE'].append(mse_rf)\n",
    "        results['Regressor']['SMAPE'].append(smape_rf)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run Monte Carlo simulation with 10 iterations\n",
    "n_iterations = 30\n",
    "results = monte_carlo_simulation(n_iterations)\n",
    "\n",
    "\n",
    "# Store the means for the comparison\n",
    "mean_mae_cal = np.mean(results['CustomAlgorithm']['MAE'])\n",
    "mean_mse_cal= np.mean(results['CustomAlgorithm']['MSE'])\n",
    "mean_smape_cal = np.mean(results['CustomAlgorithm']['SMAPE'])\n",
    "\n",
    "mean_mae_rf = np.mean(results['Regressor']['MAE'])\n",
    "mean_mse_rf = np.mean(results['Regressor']['MSE'])\n",
    "mean_smape_rf = np.mean(results['Regressor']['SMAPE'])\n",
    "\n",
    "\n",
    "# Display results\n",
    "print(\"CustomAlgorithm Results:\")\n",
    "print(f\"Mean MAE: {np.mean(results['CustomAlgorithm']['MAE'])}\")\n",
    "print(f\"Mean MSE: {np.mean(results['CustomAlgorithm']['MSE'])}\")\n",
    "print(f\"Mean SMAPE: {np.mean(results['CustomAlgorithm']['SMAPE'])}\")\n",
    "\n",
    "print(\"Base_Regressor Results:\")\n",
    "print(f\"Mean MAE: {np.mean(results['Regressor']['MAE'])}\")\n",
    "print(f\"Mean MSE: {np.mean(results['Regressor']['MSE'])}\")\n",
    "print(f\"Mean SMAPE: {np.mean(results['Regressor']['SMAPE'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25d4029-cb09-4504-aee7-af47845101d6",
   "metadata": {},
   "source": [
    "### 0.0.3 Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740aa26-9700-4b1e-a341-508a74b49276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms combined\n",
    "metrics = ['MAE', 'MSE', 'SMAPE']\n",
    "for metric in metrics:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Histogram for both Regressor and CustomAlgorithm\n",
    "    plt.hist(results['CustomAlgorithm'][metric], bins=20, alpha=0.7, color='blue', label='CustomAlgorithm', histtype = \"barstacked\")\n",
    "    plt.hist(results['Regressor'][metric], bins=20, alpha=0.5, color='orange', label='Base_Regressor', histtype = \"barstacked\")\n",
    "    \n",
    "    plt.title(f'{metric} Histogram')\n",
    "    plt.xlabel(metric)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c94fcc8-3bcb-4c57-84c9-e4179f4d8922",
   "metadata": {},
   "source": [
    "### 0.0.4 Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb078a70-2fcf-4cd5-a794-1c3c3f66b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_metrics(metric_cal, metric_rf, metric_name):\n",
    "    print('CustomAlgorithm score: %g | base Regressor score: %g' % (metric_cal, metric_rf))\n",
    "\n",
    "    if metric_cal < metric_rf:\n",
    "        print(f\"The model is more accurate with CustomAlgorithm in terms of {metric_name}.\")\n",
    "        return\n",
    "\n",
    "    elif metric_cal == metric_rf:\n",
    "        print(\"Further examination is required to determine superiority.\")\n",
    "        return\n",
    "\n",
    "    else:\n",
    "        print(f\"The CustomAlgorithm model is not better in terms of {metric_name}.\")\n",
    "        return\n",
    "        \n",
    "\n",
    "compare_metrics(mean_smape_cal, mean_smape_rf, \"SMAPE\")\n",
    "compare_metrics(mean_mae_cal, mean_mae_rf, \"MAE\")\n",
    "compare_metrics(mean_mse_cal, mean_mse_rf, \"MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de848e-47ae-4f3b-96fc-3d7ba336e920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
